{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "297f395d",
   "metadata": {},
   "source": [
    "# W3 Inclass Activity\n",
    "## Ivan Zepeda\n",
    "### C0883949"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158eca6a",
   "metadata": {},
   "source": [
    "### ***what are two main challenges  of working with textual data**\n",
    "When working with textual data, two main challenges often arise:\n",
    "\n",
    "**Data Preprocessing**: Textual data is typically unstructured and requires preprocessing before it can be effectively analyzed. Some common challenges in data preprocessing include:\n",
    "\n",
    "a. Tokenization: Breaking down the text into individual units such as words or sentences.\n",
    "b. Stop Word Removal: Identifying and removing commonly used words that do not carry significant meaning, such as \"a,\" \"the,\" or \"and.\"\n",
    "c. Stemming and Lemmatization: Reducing words to their base or root form to handle variations (e.g., \"running\" and \"ran\" to \"run\").\n",
    "d. Handling Special Characters and Symbols: Dealing with punctuation marks, emoticons, URLs, or other non-alphabetic characters.\n",
    "e. Dealing with Noisy Data: Handling misspelled words, abbreviations, or inconsistent capitalization.\n",
    "\n",
    "These preprocessing challenges are crucial to ensure the quality and accuracy of subsequent analyses.\n",
    "\n",
    "**Textual Representation**: Once the data is preprocessed, another challenge is representing textual data in a format suitable for machine learning algorithms or other analyses. Some common challenges include:\n",
    "\n",
    "a. Feature Extraction: Converting textual data into numerical or categorical features that algorithms can understand. This can involve techniques like Bag-of-Words, TF-IDF (Term Frequency-Inverse Document Frequency), or word embeddings such as Word2Vec or GloVe.\n",
    "b. High-Dimensionality: Textual data often leads to a high-dimensional feature space due to a large number of unique words or phrases, which can impact computational efficiency and require dimensionality reduction techniques.\n",
    "c. Contextual Understanding: Textual data often contains complex nuances, such as sarcasm, irony, or cultural references, which can be challenging for algorithms to interpret correctly.\n",
    "d. Handling Large Volumes: Textual data is often abundant, especially in applications like social media or customer feedback. Managing and processing large volumes of textual data efficiently can be computationally demanding.\n",
    "\n",
    "These challenges highlight the importance of careful data preprocessing and effective representation techniques to extract meaningful insights from textual data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78738b6f",
   "metadata": {},
   "source": [
    "### mention 3 feature selection methods\n",
    "\n",
    "Feature selection is a crucial step in machine learning to identify the most relevant and informative features for a given task. Here are three common feature selection methods:\n",
    "\n",
    "**Filter Methods**: Filter methods assess the relevance of features based on their intrinsic characteristics, independent of any specific machine learning algorithm. Some popular filter methods include:\n",
    "\n",
    "a. Pearson's Correlation: Measures the linear relationship between each feature and the target variable. Features with high correlation values are considered more relevant.\n",
    "b. Chi-Square Test: Evaluates the dependency between categorical features and the target variable. It is commonly used for feature selection in classification problems.\n",
    "c. Information Gain: Measures the reduction in entropy or uncertainty in the target variable when a feature is known. Features with high information gain are considered more important.\n",
    "\n",
    "Filter methods are computationally efficient and provide a quick way to rank features based on their individual properties.\n",
    "\n",
    "**Wrapper Methods**: Wrapper methods select features by evaluating their impact on the performance of a specific machine learning algorithm. These methods involve iteratively training the model on different feature subsets and selecting the subset that yields the best performance. Some popular wrapper methods include:\n",
    "\n",
    "a. Recursive Feature Elimination (RFE): Starts with all features, trains the model, and iteratively removes the least important feature(s) until a desired number of features remains.\n",
    "b. Forward Selection: Starts with an empty feature set and gradually adds the most relevant feature at each step based on its impact on the model performance.\n",
    "c. Genetic Algorithms: Utilizes principles inspired by biological evolution to search for an optimal subset of features. It involves creating a population of potential feature subsets and iteratively selecting, mutating, and recombining them to find the best subset.\n",
    "\n",
    "Wrapper methods can provide more accurate feature subsets but can be computationally expensive, especially with a large number of features.\n",
    "\n",
    "**Embedded Methods**: Embedded methods incorporate feature selection within the training process of the machine learning algorithm itself. These methods select features based on their importance or contribution to the model during training. Some common embedded methods include:\n",
    "\n",
    "a. Lasso (Least Absolute Shrinkage and Selection Operator): Regularizes the model by adding a penalty term based on the absolute value of the feature coefficients, encouraging sparse solutions and effectively performing feature selection.\n",
    "b. Random Forest Feature Importance: Measures the importance of each feature by evaluating how much the predictive accuracy of a random forest model decreases when that feature is randomly permuted.\n",
    "c. Gradient Boosting Feature Importance: Similar to Random Forest, gradient boosting algorithms can estimate feature importance based on the contribution of each feature to the overall improvement in model performance.\n",
    "\n",
    "Embedded methods combine feature selection with model training, providing a balance between accuracy and computational efficiency.\n",
    "\n",
    "These feature selection methods help to improve model performance, reduce overfitting, and enhance interpretability by focusing on the most relevant features for the task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8fd374",
   "metadata": {},
   "source": [
    "### What are some NLP Tools\n",
    "\n",
    "popular NLP tools:\n",
    "\n",
    "**NLTK (Natural Language Toolkit)**: NLTK is a widely used Python library for NLP. It provides a comprehensive suite of libraries and programs for tasks such as tokenization, stemming, lemmatization, part-of-speech tagging, syntactic parsing, and more. NLTK also includes various corpora and lexicons for training and evaluation.\n",
    "\n",
    "**spaCy**: spaCy is another popular open-source NLP library written in Python. It offers efficient and fast tokenization, part-of-speech tagging, named entity recognition, dependency parsing, and sentence segmentation. spaCy is known for its high performance, easy-to-use API, and support for multiple languages.\n",
    "\n",
    "**Gensim**: Gensim is a Python library for topic modeling, document similarity, and word embedding techniques. It provides an implementation of popular algorithms like Latent Semantic Analysis (LSA), Latent Dirichlet Allocation (LDA), and Word2Vec. Gensim is particularly useful for extracting semantic meanings from text and building vector representations.\n",
    "\n",
    "**CoreNLP**: CoreNLP is a Java-based NLP toolkit developed by Stanford University. It offers a wide range of NLP functionalities, including tokenization, sentence splitting, part-of-speech tagging, named entity recognition, sentiment analysis, coreference resolution, and dependency parsing. CoreNLP is highly regarded for its accuracy and robustness.\n",
    "\n",
    "**WordNet**: WordNet is a lexical database that provides a large collection of English words and their semantic relationships. It groups words into synsets (sets of synonyms) and organizes them based on their lexical and semantic properties. WordNet is commonly used for tasks like word sense disambiguation, synonym identification, and semantic similarity calculations.\n",
    "\n",
    "**BERT (Bidirectional Encoder Representations from Transformers)**: BERT is a powerful language model developed by Google that has revolutionized many NLP tasks. It is pre-trained on large-scale corpora and can be fine-tuned for specific downstream tasks such as text classification, named entity recognition, question answering, and more. BERT provides contextually rich word representations and has achieved state-of-the-art performance on several benchmarks.\n",
    "\n",
    "**OpenAI GPT (Generative Pre-trained Transformer)**: GPT is another influential language model developed by OpenAI. GPT models are trained on massive amounts of text data and can generate coherent and contextually relevant text. GPT models have been widely used for tasks such as text generation, text completion, summarization, and conversational agents.\n",
    "\n",
    "These are just a few examples of NLP tools available. Each tool has its own set of features and strengths, allowing developers and researchers to leverage them for a wide range of NLP applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ec746a",
   "metadata": {},
   "source": [
    "### Coding - Activity\n",
    "\n",
    "  \n",
    "    \n",
    "Write a Python program to find three numbers from an array such that the sum of three equals to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b450b684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list: [[-1, 0, 1], [-1, 2, -1], [0, 1, -1]]\n"
     ]
    }
   ],
   "source": [
    "# in: [-1,0,1,2,-1,4]\n",
    "# out: [[-1,-1,2],[-1,0,1]]\n",
    "    \n",
    "def sum3(nums):\n",
    "    lista=[]\n",
    "    for i in range(len(nums)-2):\n",
    "        for j in range(i+1,len(nums)-1):\n",
    "            for k in range(j+1,len(nums)):\n",
    "#                 print(f\"i:{i} nums[i]:{nums[i]} | j:{j} nums[j]:{nums[j]} | k:{k} nums[k]:{nums[k]} \\t total={(nums[i]+nums[j]+nums[k])}\")\n",
    "                if((nums[i]+nums[j]+nums[k])==0):\n",
    "                    lista.append([nums[i],nums[j],nums[k]])\n",
    "    return lista\n",
    "r=sum3([-1,0,1,2,-1,4])\n",
    "print(\"list:\",r)\n",
    "\n",
    "#list of lists are not hashables, it requires to be converted to something like a tuple\n",
    "# print(\"set:\",set(r))\n",
    "# print(\"set:\", set(tuple(x) for x in r))  # Convert sublists to tuples before creating a set\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfaba89",
   "metadata": {},
   "source": [
    "Write a Python program to find the single element in a list   \n",
    "where every element appears three times except for one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b58a6593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# in [5,3,4,3,5,5,3]\n",
    "# out:4\n",
    "\n",
    "def loner(nums):\n",
    "  #Approach1 use dictionary\n",
    "  #approach2 use count\n",
    "    for n in nums:\n",
    "        if(nums.count(n)==1):\n",
    "            print(n)\n",
    "    \n",
    "    \n",
    "    \n",
    "loner([5,3,4,3,5,5,3])\n",
    "# print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75972a3",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cfb45554",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data=[\"   Interrobang. By Aishwarya Henriette\",\"Parking And Going. By KArl Gautier\",\"   Today is The night. By Jarek Prakash    \"          ]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fdd3c82f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Interrobang. By Aishwarya Henriette',\n",
       " 'Parking And Going. By KArl Gautier',\n",
       " 'Today is The night. By Jarek Prakash']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strip_whitespaces=[string.strip() for string in text_data]\n",
    "strip_whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf16d69b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
